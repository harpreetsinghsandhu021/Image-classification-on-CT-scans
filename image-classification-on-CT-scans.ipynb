{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Image Classification from CT Scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import os \n",
    "import zipfile \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the MosMedData: Chest CT Scans with Covid-19 Related Findings\n",
    "In this example, we use a subset of the\n",
    "[MosMedData: Chest CT Scans with COVID-19 Related Findings](https://www.medrxiv.org/content/10.1101/2020.05.20.20100362v1).\n",
    "This dataset consists of lung CT scans with COVID-19 related findings, as well as without such findings.\n",
    "\n",
    "We will be using the associated radiological findings of the CT scans as labels to build\n",
    "a classifier to predict presence of viral pneumonia.\n",
    "Hence, the task is a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download normal CT scans.\n",
    "# url = \"https://github.com/hasibzunair/3D-image-classification-tutorial/releases/download/v0.2/CT-0.zip\"\n",
    "# filename = os.path.join(os.getcwd(), \"CT-0.zip\")\n",
    "# tf.keras.utils.get_file(cache_dir=filename, origin=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download abnormal CT scans.\n",
    "# url = \"https://github.com/hasibzunair/3D-image-classification-tutorial/releases/download/v0.2/CT-23.zip\"\n",
    "# filename = os.path.join(os.getcwd(), \"CT-23.zip\")\n",
    "# tf.keras.utils.get_file(cache_dir=filename, origin=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with zipfile.ZipFile('CT-0.zip','r') as z_fp: \n",
    "#     z_fp.extractall('./MosMedData')\n",
    "\n",
    "# with zipfile.ZipFile('CT-23.zip','r') as z_fp: \n",
    "#     z_fp.extractall('./MosMedData')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data and Prepocessing \n",
    "The files are provided in Nifti format with the extension .nii. To read the\n",
    "scans, we use the `nibabel` package.\n",
    "CT scans store raw voxel intensity in Hounsfield units (HU). They range from -1024 to above 2000 in this dataset.\n",
    "Above 400 are bones with different radiointensity, so this is used as a higher bound. A threshold\n",
    "between -1000 and 400 is commonly used to normalize CT scans.\n",
    "\n",
    "To process the data, we do the following:\n",
    "\n",
    "* We first rotate the volumes by 90 degrees, so the orientation is fixed\n",
    "* We scale the HU values to be between 0 and 1.\n",
    "* We resize width, height and depth.\n",
    "\n",
    "Here we define several helper functions to process the data. These functions\n",
    "will be used when building training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib \n",
    "from scipy import ndimage\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nifti_file(filepath): \n",
    "    \"Read and Load volume\"\n",
    "    scan = nib.load(filepath)\n",
    "    scan = scan.get_fdata()\n",
    "    return scan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(volume): \n",
    "    \"\"\"Normalize the volume\"\"\"\n",
    "    min = -1000\n",
    "    max = 400\n",
    "    volume[volume < min] = min\n",
    "    volume[volume > max] = max\n",
    "\n",
    "    volume = (volume - min) / (max - min)\n",
    "    volume = volume.astype('float32')\n",
    "    return volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_volume(img):  \n",
    "    \"Resize across z-axis\"\n",
    "    # Set the desired dimensions of the image\n",
    "    desired_depth = 64\n",
    "    desired_width = 128\n",
    "    desired_height = 128\n",
    "\n",
    "    # Get the current dimensions of the image\n",
    "    current_depth = img.shape[-1]\n",
    "    current_width = img.shape[0]\n",
    "    current_height = img.shape[1]\n",
    "\n",
    "    # Calculate the scaling factors for each dimension\n",
    "    depth = current_depth / desired_depth\n",
    "    width = current_width / desired_width\n",
    "    height = current_height / desired_height\n",
    "\n",
    "    depth_factor = 1 / depth\n",
    "    width_factor = 1 / width\n",
    "    height_factor = 1 / height\n",
    "\n",
    "    # Rotate  the image by 90 deg\n",
    "    img = ndimage.rotate(img, 90, reshape=False)\n",
    "    # Resize the image using the calculated scaling factors\n",
    "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scan(path): \n",
    "    \"Read and Resize Volume\"\n",
    "    volume = read_nifti_file(path)\n",
    "    volume = resize_volume(volume)\n",
    "    volume = resize_volume(volume)\n",
    "    return volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the paths of the CT scans from the class directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_scan_paths = [\n",
    "    os.path.join(os.getcwd(), \"MosMedData/CT-0\", x)\n",
    "    for x in os.listdir(\"MosMedData/CT-0\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT scans with normal lung tissue: 100\n",
      "CT scans with abnormal lung tissue: 100\n"
     ]
    }
   ],
   "source": [
    "abnormal_scan_paths = [\n",
    "    os.path.join(os.getcwd(), \"MosMedData/CT-23\", x)\n",
    "    for x in os.listdir(\"MosMedData/CT-23\")\n",
    "]\n",
    "print(\"CT scans with normal lung tissue: \" + str(len(normal_scan_paths)))\n",
    "print(\"CT scans with abnormal lung tissue: \" + str(len(abnormal_scan_paths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data \n",
    "Read the scans from the class directories and assign labels. Downsample the scans to have\n",
    "shape of 128x128x64. Rescale the raw HU values to the range 0 to 1.\n",
    "Lastly, split the dataset into train and validation subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train and validation are 140 and 60.\n"
     ]
    }
   ],
   "source": [
    "normal_scans = np.array([process_scan(path) for path in normal_scan_paths])\n",
    "abnormal_scans = np.array([process_scan(path) for path in abnormal_scan_paths])\n",
    "\n",
    "normal_labels = np.array([0 for _ in range(len(normal_scans))])\n",
    "abnormal_labels = np.array([1 for _ in range(len(abnormal_scans))])\n",
    "\n",
    "split_ratio = 0.7\n",
    "\n",
    "split_index_normal = int(len(normal_scans) * split_ratio)\n",
    "split_index_abnormal = int(len(abnormal_scans) * split_ratio)\n",
    "\n",
    "X_train = np.concatenate((normal_scans[:split_index_normal],abnormal_scans[:split_index_abnormal]), axis=0)\n",
    "y_train = np.concatenate((normal_labels[:split_index_normal], abnormal_labels[:split_index_abnormal]), axis=0)\n",
    "x_val = np.concatenate((normal_scans[split_index_normal:], abnormal_scans[split_index_abnormal:]), axis=0)\n",
    "y_val = np.concatenate((normal_labels[:split_index_normal:], abnormal_labels[split_index_abnormal:]), axis=0)\n",
    "\n",
    "print(\n",
    "    \"Number of samples in train and validation are %d and %d.\"\n",
    "    % (X_train.shape[0], x_val.shape[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "The CT scans also augmented by rotating at random angles during training. Since\n",
    "the data is stored in rank-3 tensors of shape `(samples, height, width, depth)`,\n",
    "we add a dimension of size 1 at axis 4 to be able to perform 3D convolutions on\n",
    "the data. The new shape is thus `(samples, height, width, depth, 1)`. There are\n",
    "different kinds of preprocessing and augmentation techniques out there,\n",
    "this example shows a few simple ones to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(volume): \n",
    "    \"Rotate the volume by few degrees\"\n",
    "    \n",
    "    def scipy_rotate(volume): \n",
    "        angles = [-20, -10, -5, 5, 10, 20]\n",
    "        angle = random.choice(angles) # pick angle at random\n",
    "\n",
    "        volume = ndimage.rotate(volume, angle, reshape=False)\n",
    "        volume[volume < 0] = 0\n",
    "        volume[volume > 1] = 1\n",
    "        return volume\n",
    "    \n",
    "    augmented_volume = tf.numpy_function(scipy_rotate, [volume], tf.float32)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-series",
   "language": "python",
   "name": "tensorflow-series"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
